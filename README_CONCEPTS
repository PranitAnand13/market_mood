# Concepts Learned – Market Mood and Moves

This document summarizes the conceptual understanding gained during the
*Market Mood and Moves* project under WiDS 5.0. The focus has been on learning
the theoretical foundations behind sentiment-driven financial systems and
understanding how these ideas translate into realistic data pipelines.

The project emphasized correctness, causality, and system design over raw
predictive performance, aligning closely with industry-oriented analytics
thinking.

---

## Behavioral Finance and Market Sentiment

Classical financial theory assumes that markets are efficient and that prices
fully reflect all available information. Behavioral finance challenges this
assumption by incorporating psychological and social factors into market
behavior. Investors often exhibit cognitive biases such as overconfidence,
loss aversion, and herd behavior, which can lead to short-term mispricing.

Herd behavior refers to the tendency of investors to follow the actions of the
majority rather than relying on independent analysis. News events play a key
role in triggering herd behavior, as widely disseminated information can
simultaneously influence a large number of market participants.

Throughout the project, news sentiment is treated as a proxy for collective
investor psychology rather than long-term fundamental value.

---

## Market Sentiment as a Quantitative Signal

Market sentiment represents the overall emotional tone of market participants
toward a company or the broader market. Although sentiment is qualitative by
nature, it can be quantified by applying Natural Language Processing (NLP)
techniques to financial text such as news headlines.

In this project, sentiment scores extracted from news are viewed as short-term
signals that may precede price movements, particularly around earnings,
macroeconomic announcements, or unexpected corporate events.

---

## Data Ingestion and the Role of APIs

Data ingestion refers to the systematic process of collecting raw data from
external sources before any analysis is performed. APIs such as NewsAPI were
studied to understand how live financial news can be fetched programmatically,
validated, and stored in a structured format.

Key learnings include:
- Separation of raw data ingestion from downstream processing
- Secure handling of API keys
- Awareness of rate limits and result caps in free-tier APIs

These constraints highlighted the practical differences between academic models
and real-world financial data systems.

---

## Evolution of Word Representations in NLP

Early NLP models relied on static word embeddings such as Word2Vec and GloVe,
which assign a single vector representation to each word in the vocabulary.
These models suffer from the problem of polysemy, where a word can have multiple
meanings depending on context.

For example, the word “bank” may refer to a financial institution or a riverbank.
Static embeddings collapse these meanings into a single vector, leading to
semantic ambiguity.

This limitation is particularly severe in financial text, where context is
critical.

---

## Contextual Embeddings and the BERT Breakthrough

BERT (Bidirectional Encoder Representations from Transformers) introduced
contextual embeddings, where a word’s representation depends on its surrounding
context rather than being fixed.

This allows the same word to have different vector representations in different
sentences, resolving many limitations of static embeddings. This concept forms
the backbone of modern financial NLP models.

---

## BERT Input Representation

BERT constructs its input representation as the sum of:
- **Token embeddings** (via WordPiece tokenization)
- **Positional embeddings** (to encode word order)
- **Segment embeddings** (to distinguish sentence pairs)

WordPiece tokenization is particularly effective for financial text, as it
handles rare company names, tickers, and compound financial terms gracefully.

---

## Transformer Encoder and Self-Attention

The Transformer encoder relies on self-attention mechanisms to model
relationships between all words in a sentence simultaneously. Each token can
attend to every other token, allowing the model to capture nuanced dependencies.

This mechanism is especially powerful for financial headlines, where sentiment
often depends on numerical qualifiers, negations, and contextual phrasing rather
than individual words.

---

## Pre-Training Objectives in BERT

BERT is trained using two key objectives:

1. **Masked Language Modeling (MLM)**  
   Random tokens are masked and predicted, encouraging deep contextual learning.

2. **Next Sentence Prediction (NSP)**  
   The model learns whether two sentences logically follow one another.

These objectives enable BERT to learn both intra-sentence and inter-sentence
relationships.

---

## Domain Shift in Financial Language

A major challenge in applying NLP to finance is **domain shift**. Financial
language differs significantly from general English. Words such as “liability,”
“loss,” or “volatile” may carry technical or neutral meanings in finance but are
interpreted negatively in everyday language.

This mismatch motivates the use of domain-adapted models rather than generic
sentiment tools.

---

## FinBERT and Domain Adaptation

FinBERT is a domain-adapted version of BERT designed specifically for financial
sentiment analysis. It undergoes additional pre-training on large financial
corpora before being fine-tuned on labeled financial sentiment datasets.

This adaptation allows FinBERT to better capture the semantics and structure of
financial language, making it more suitable for market sentiment analysis than
general-purpose models.

---

## Sentiment Classification and Probabilistic Outputs

For sentiment classification, a classification head is attached to the `[CLS]`
token. The model outputs logits for three classes: Positive, Negative, and
Neutral. These are converted into probabilities using a softmax function.

Probabilistic sentiment outputs are especially useful in finance, as they allow
aggregation across multiple articles and provide a measure of confidence rather
than binary decisions.

---

## Look-Ahead Bias and Temporal Alignment

Look-ahead bias occurs when future information is inadvertently incorporated
into past observations. In sentiment-based systems, this often arises when news
published after market close is incorrectly aligned with the same trading day.

A key learning of the project was the importance of mapping news timestamps to
correct trading days based on market hours and weekends to preserve causality.

---

## Time Series Modeling and Temporal Dependency

Financial markets are inherently sequential. Prices and sentiment signals are
not independent observations but evolve over time. This violates the I.I.D.
assumption underlying many classical machine learning models.

Sequence-aware models such as Recurrent Neural Networks (RNNs) and Long
Short-Term Memory (LSTM) networks were studied to understand how historical
information can be incorporated into predictions.

LSTMs address the vanishing gradient problem of standard RNNs through gated
memory mechanisms, making them suitable for capturing longer-term dependencies
in financial time series.

---

## Multimodal Market Representation

An important conceptual takeaway is that financial markets are multimodal.
Prices reflect realized outcomes, while sentiment reflects expectations,
reactions, and uncertainty.

Combining price-based features with aggregated sentiment signals allows models
to reason about both historical behavior and market mood, even if sentiment
alone is noisy.

---

## System-Level Thinking and Integration

Later stages of the project emphasized system-level design over isolated models.
Key insights include:
- Financial ML pipelines are data engineering problems as much as modeling ones
- Temporal correctness is more important than model complexity
- Data scarcity and noise strongly influence deep learning effectiveness
- Modularity improves reproducibility and debugging

---

## Summary of Learnings

Across all weeks, the project emphasized understanding how behavioral finance,
NLP, and time series modeling interact within realistic constraints. Rather than
treating models as black boxes, the focus was on causality, alignment, and
interpretability.

The project reinforced that successful sentiment-driven financial systems depend
as much on data handling and system design as on machine learning algorithms.
